# Temporal-Phase Spin Retrieval System

A novel retrieval algorithm that encodes time as an angular spin state on the unit circle, enabling smooth temporal zoom without model retraining.

## ğŸ¯ Core Concept

Traditional retrieval systems treat time as a scalar feature or discrete bucket. This system represents time as a **continuous angular coordinate** on the unit circle:

```
Ï† = 2Ï€ Ã— (t - tâ‚€) / T

spin_vector = [cos(Ï†), sin(Ï†)]

full_embedding = [semantic_embedding, spin_vector]
```

### Key Innovation: No Model Retraining Required

The semantic embedding model is **frozen**. Time encoding happens post-hoc in the vector space via geometric augmentation, making this approach:

- âœ… Model-agnostic (works with any embedding model)
- âœ… Efficient (no retraining overhead)
- âœ… Interpretable (phase angles have clear geometric meaning)
- âœ… Controllable (Î² parameter adjusts temporal focus at runtime)

## ğŸ”¬ How It Works

### Ingestion Pipeline

1. **Timestamp Extraction**: Parse timestamps from document text using regex patterns and dateutil
   - Recognizes formats like "for the period ended 31 December 2019"
   - Falls back to file metadata or ingestion time

2. **Semantic Embedding**: Obtain text embedding from LlamaStack Model Gateway
   - Uses registered embedding models (e.g., `text-embedding-v1`)
   - No special temporal training needed

3. **Spin Encoding**: Convert timestamp to 2D spin vector
   ```python
   fraction = ((timestamp - tâ‚€) / period) % 1.0
   Ï† = 2Ï€ Ã— fraction
   spin = [cos(Ï†), sin(Ï†)]
   ```

4. **Concatenation**: Combine semantic + spin into full embedding
   ```python
   full_embedding = [semantic_embedding..., spin_vector[0], spin_vector[1]]
   ```

5. **Storage**: Index in vector database (PGVector, Chroma, or in-memory)

### Retrieval Algorithm: Two-Pass Temporal Zoom

#### Pass 1: Coarse Recall (Broad Semantic Search)

```python
query_full = [query_semantic, Î» Ã— query_spin]  # Small Î» â‰ˆ 0.1
candidates = vector_db.search(query_full, top_k=50)
```

Uses small Î» to perform broad semantic search with minor temporal weighting.

#### Pass 2: Temporal Zoom Re-ranking

```python
for doc in candidates:
    Î”Ï† = angular_difference(Ï†_query, Ï†_doc)  # Shortest arc on circle
    temporal_alignment = exp(-Î² Ã— (Î”Ï†)Â²)
    score = semantic_similarity Ã— temporal_alignment
```

Recomputes scores using **Î² (zoom factor)** to control temporal focus:

- **Î² = 0**: Pure semantic search (time ignored)
- **Î² = 1**: Slight temporal preference  
- **Î² = 5**: Balanced semantic + temporal
- **Î² = 10**: Strong temporal focus
- **Î² = 20+**: Very sharp temporal filter (phase-locked)

The temporal alignment factor `exp(-Î² Ã— (Î”Ï†)Â²)`:
- Equals 1.0 when phases align perfectly (Î”Ï† = 0)
- Decays smoothly as phases diverge
- Decays faster with larger Î² (sharper focus)

## ğŸš€ Quick Start

### Installation

```bash
# Clone or create project directory
cd embeddingspin

# Install dependencies
pip install -r requirements.txt
```

### Run Demo (Mock Embeddings)

The demo uses mock embeddings for fast, standalone testing:

```bash
# Full interactive demo
python demo.py

# Custom query with specific Î²
python demo.py --query "IBM cloud strategy" --timestamp 2019-06-30 --beta 10.0

# Show Î² parameter sweep
python demo.py --beta-sweep
```

### Run API Server

```bash
# Start FastAPI server with mock embeddings
python api.py

# Visit interactive docs
open http://localhost:8080/docs
```

**Example API Request:**

```bash
curl -X POST "http://localhost:8080/temporal_search" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "IBM revenue 2016",
    "query_timestamp": "2016-06-30T00:00:00Z",
    "beta": 5.0,
    "top_k": 10
  }'
```

### Production Setup (LlamaStack + PGVector)

```bash
# Set environment variables
export USE_MOCK_EMBEDDINGS=false
export LLAMASTACK_URL=http://localhost:8000
export EMBEDDING_MODEL=text-embedding-v1
export VECTOR_STORE=pgvector
export DATABASE_URL=postgresql://user:pass@localhost:5432/vectordb

# Run API server
python api.py
```

## ğŸ“Š Demo Dataset

Includes 10 mock IBM financial reports (2015-2024) with:
- Realistic revenue and profit figures
- Strategic initiatives per year (Watson AI, Red Hat, hybrid cloud, quantum)
- Natural language suitable for semantic search
- Explicit date markers for timestamp extraction

**Example Query Demonstrations:**

| Query | Timestamp | Î² | Expected Behavior |
|-------|-----------|---|-------------------|
| "IBM revenue" | 2016-06-30 | 5.0 | Prioritizes 2016 report |
| "IBM cloud strategy" | 2019-12-31 | 10.0 | Focuses on Red Hat acquisition era (2019-2020) |
| "IBM quantum computing" | 2024-06-30 | 5.0 | Highlights recent 2024 developments |

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     User Query + Timestamp                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LlamaStack Embedding Client                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Model Gateway API â†’ text-embedding-v1                   â”‚   â”‚
â”‚  â”‚  Returns: semantic_embedding (e.g., 384-dim)            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Temporal Spin Encoder                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Ï† = 2Ï€ Ã— (timestamp - tâ‚€) / period                     â”‚   â”‚
â”‚  â”‚  spin = [cos(Ï†), sin(Ï†)]                                â”‚   â”‚
â”‚  â”‚  query_full = [semantic, Î» Ã— spin]                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PASS 1: Coarse Recall (Î» = 0.1)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Vector DB Search (cosine similarity)                    â”‚   â”‚
â”‚  â”‚  Retrieve top-K candidates (broad semantic search)       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PASS 2: Temporal Zoom Re-ranking                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  For each candidate:                                     â”‚   â”‚
â”‚  â”‚    Î”Ï† = angular_difference(Ï†_query, Ï†_doc)              â”‚   â”‚
â”‚  â”‚    alignment = exp(-Î² Ã— (Î”Ï†)Â²)                          â”‚   â”‚
â”‚  â”‚    score = semantic_sim Ã— alignment                      â”‚   â”‚
â”‚  â”‚  Sort by score, return top-k                             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                          Ranked Results
```

## ğŸ“ Project Structure

```
embeddingspin/
â”œâ”€â”€ temporal_spin.py        # Core: spin encoding, timestamp extraction
â”œâ”€â”€ llamastack_client.py    # LlamaStack API wrapper + mock client
â”œâ”€â”€ vector_store.py         # Vector DB abstraction (Memory/Chroma/PGVector)
â”œâ”€â”€ ingestion.py            # Document ingestion pipeline
â”œâ”€â”€ retrieval.py            # Two-pass retrieval algorithm
â”œâ”€â”€ demo_data.py            # Mock IBM reports generator
â”œâ”€â”€ demo.py                 # CLI demo script
â”œâ”€â”€ api.py                  # FastAPI REST API
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md               # This file
```

## ğŸ”§ Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `USE_MOCK_EMBEDDINGS` | `true` | Use mock embeddings for testing |
| `LLAMASTACK_URL` | `http://localhost:8000` | LlamaStack API base URL |
| `LLAMASTACK_API_KEY` | - | Optional API key |
| `EMBEDDING_MODEL` | `text-embedding-v1` | Embedding model name |
| `VECTOR_STORE` | `memory` | Vector store type: `memory`, `chroma`, `pgvector` |
| `CHROMA_PERSIST_DIR` | `./chroma_db` | Chroma persistence directory |
| `DATABASE_URL` | - | PostgreSQL connection string (for pgvector) |
| `LOAD_DEMO_DATA` | `true` | Auto-load IBM demo reports on startup |
| `PORT` | `8080` | API server port |
| `HOST` | `0.0.0.0` | API server host |

### Temporal Encoding Parameters

```python
T0_EPOCH = datetime(2010, 1, 1)      # Base epoch
PERIOD_SECONDS = 365.25 * 24 * 3600 * 10  # 10-year period
```

Adjustable in code for different temporal scales (daily, monthly, yearly cycles).

## ğŸ“ Use Cases

### 1. Financial Report Search
Query: "Q4 earnings 2019"  
â†’ Retrieves reports from Q4 2019, with Î² controlling temporal window

### 2. Legal Document Retrieval
Query: "GDPR compliance for the period ended 2020"  
â†’ Finds documents from 2020 compliance period

### 3. News Archive Search
Query: "COVID-19 vaccine development December 2020"  
â†’ Focuses on December 2020 news articles

### 4. Medical Records
Query: "patient symptoms January 2023"  
â†’ Retrieves records from January 2023 visit

### 5. Code Repository Search
Query: "authentication bug fix"  
Timestamp: Last month  
â†’ Prioritizes recent commits

## ğŸ”¬ Advanced Features

### Beta Sweep API

Compare results across multiple Î² values:

```python
POST /beta_sweep
{
  "query": "IBM AI strategy",
  "query_timestamp": "2019-06-30T00:00:00Z",
  "beta_values": [0, 1, 5, 10, 20],
  "top_k": 5
}
```

Returns results for each Î², showing smooth transition from semantic to temporal focus.

### Custom Timestamp Extraction

Add custom regex patterns for domain-specific date formats:

```python
from temporal_spin import DATE_PATTERNS

# Add custom pattern
DATE_PATTERNS.append(r'report\s+date:\s+(\d{4}-\d{2}-\d{2})')
```

### Multiple Embedding Models

Switch models without changing spin encoding:

```python
# Use different model
client = LlamaStackEmbeddingClient(
    model_name="nomic-embed-text-v1.5"
)
```

Spin encoding works with any embedding model!

## ğŸ“ˆ Performance

### Ingestion
- **Single document**: ~50-100ms (embedding + spin encoding + DB insert)
- **Batch (100 docs)**: ~2-5s (batched embeddings amortize overhead)

### Retrieval
- **Pass 1 (coarse recall)**: ~10-50ms (vector DB search)
- **Pass 2 (re-ranking)**: ~1-5ms (in-memory computation)
- **Total**: ~15-55ms for typical queries

### Scalability
- **In-Memory**: < 10k documents
- **Chroma**: < 1M documents
- **PGVector**: 10M+ documents (with proper indexing)

## ğŸ§ª Testing

```bash
# Run demo with mock data
python demo.py

# Test specific query
python demo.py --query "test query" --timestamp 2020-01-01 --beta 5.0

# Show Î² sweep
python demo.py --beta-sweep

# Test API endpoints
pytest tests/  # (if you add tests/)
```

## ğŸ¤ Integration with Red Hat AI 3 (LlamaStack)

This system is designed for Red Hat AI 3 environments:

1. **Model Gateway**: Automatically discovers registered embedding models
2. **Vector Store**: Works with PGVector (often bundled with LlamaStack)
3. **API**: FastAPI server integrates with existing services
4. **Scalability**: Horizontal scaling via stateless API design

**Deployment:**

```bash
# In your LlamaStack environment
pip install -r requirements.txt

# Configure
export USE_MOCK_EMBEDDINGS=false
export LLAMASTACK_URL=$MODEL_GATEWAY_URL
export VECTOR_STORE=pgvector
export DATABASE_URL=$POSTGRES_CONNECTION_STRING

# Run
python api.py
```

## ğŸ“š References & Theory

### Why Spin Encoding?

**Circular representation** of time provides several advantages:

1. **Periodicity**: Natural for recurring patterns (fiscal years, seasons)
2. **Continuity**: Smooth interpolation between timestamps
3. **Bounded**: Always 2D, regardless of time range
4. **Interpretable**: Angular difference has geometric meaning

### Mathematical Foundation

The temporal alignment factor uses a Gaussian-like kernel in phase space:

```
alignment(Î”Ï†; Î²) = exp(-Î² Ã— (Î”Ï†)Â²)
```

Properties:
- Maximum = 1 when Î”Ï† = 0 (perfect alignment)
- Decays to â‰ˆ0.37 at Î”Ï† = 1/âˆšÎ² (characteristic width)
- At Î² = 10: 95% weight within Â±0.44 radians (Â±25Â°)
- At Î² = 20: 95% weight within Â±0.31 radians (Â±18Â°)

### Comparison to Alternatives

| Approach | Pros | Cons |
|----------|------|------|
| **Scalar timestamp** | Simple | Doesn't capture periodicity |
| **Discrete buckets** | Interpretable | Hard boundaries, no interpolation |
| **Learned temporal embeddings** | Flexible | Requires retraining, less interpretable |
| **Spin encoding (ours)** | No retraining, interpretable, periodic | Assumes periodic patterns |

## ğŸ› Troubleshooting

### "Failed to get embeddings from LlamaStack"

- Check `LLAMASTACK_URL` is correct
- Verify embedding model is registered: `curl $LLAMASTACK_URL/v1/models`
- Try with `USE_MOCK_EMBEDDINGS=true` to isolate issue

### "ImportError: No module named 'chromadb'"

```bash
pip install chromadb
```

### "No documents found"

- Ensure demo data is loaded: `LOAD_DEMO_DATA=true`
- Or manually ingest: `POST /ingest`

### Results don't vary with Î²

- Check timestamps are properly parsed (not all defaulting to same time)
- Verify Î² is being passed correctly in API request
- Try larger Î² values (10-20) for sharper focus

## ğŸ“„ License

MIT License - See LICENSE file

## ğŸ‘¤ Author

Robby Brodie  
For questions or collaboration: robbytherobot@redhat.com

## ğŸ™ Acknowledgments

- Red Hat AI 3 (LlamaStack) team for Model Gateway API
- PGVector and Chroma DB for vector search capabilities
- Community contributors to dateutil, FastAPI, and related libraries

---

**Ready to revolutionize temporal retrieval?** ğŸš€

Start with: `python demo.py`

